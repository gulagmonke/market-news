"""
Market News Aggregator - Backend
Run with: python app.py
Requires: pip install feedparser flask flask-cors vaderSentiment apscheduler
"""

import feedparser
import hashlib
import json
import re
import sqlite3
import threading
from datetime import datetime, timezone
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from apscheduler.schedulers.background import BackgroundScheduler
import os

app = Flask(__name__, static_folder=".")
CORS(app)
analyzer = SentimentIntensityAnalyzer()
DB_PATH = "news.db"
LOCK = threading.Lock()

# â”€â”€ RSS Feed Sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSS_FEEDS = {
    "Reuters Business":     "https://feeds.reuters.com/reuters/businessNews",
    "Reuters Markets":      "https://feeds.reuters.com/reuters/marketsNews",
    "Yahoo Finance":        "https://finance.yahoo.com/news/rssindex",
    "MarketWatch":          "https://feeds.marketwatch.com/marketwatch/marketpulse/",
    "CNBC Economy":         "https://www.cnbc.com/id/20910258/device/rss/rss.html",
    "CNBC Finance":         "https://www.cnbc.com/id/10000664/device/rss/rss.html",
    "CNBC Earnings":        "https://www.cnbc.com/id/10001147/device/rss/rss.html",
    "Financial Times":      "https://www.ft.com/rss/home/us",
    "The Economist":        "https://www.economist.com/finance-and-economics/rss.xml",
    "ForexLive":            "https://www.forexlive.com/feed/news",
    "Calculated Risk":      "https://feeds.feedburner.com/calculatedrisk/",
    "CoinDesk":             "https://www.coindesk.com/arc/outboundfeeds/rss/",
    "Cointelegraph":        "https://cointelegraph.com/rss",
    "Kitco Gold":           "https://www.kitco.com/rss/news.xml",
}

# â”€â”€ Category Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORY_KEYWORDS = {
    "macro":       ["fed", "federal reserve", "interest rate", "inflation", "gdp", "cpi", "ppi",
                    "unemployment", "jobs", "recession", "economy", "fiscal", "monetary", "treasury",
                    "deficit", "debt", "central bank", "powell", "yellen", "fomc", "tariff", "trade war"],
    "equities":    ["stock", "equity", "equities", "nasdaq", "s&p", "dow", "nyse", "earnings",
                    "ipo", "shares", "dividend", "market cap", "bull", "bear", "rally", "selloff",
                    "wall street", "index", "etf"],
    "crypto":      ["bitcoin", "btc", "ethereum", "eth", "crypto", "blockchain", "defi", "nft",
                    "altcoin", "binance", "coinbase", "web3", "stablecoin", "solana", "ripple", "xrp"],
    "commodities": ["oil", "crude", "brent", "wti", "gold", "silver", "copper", "commodity",
                    "commodities", "natural gas", "wheat", "corn", "opec", "energy", "metals",
                    "platinum", "palladium", "futures"],
}

# â”€â”€ S&P 500 Tickers (subset for extraction) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SP500_TICKERS = set([
    "AAPL","MSFT","AMZN","NVDA","GOOGL","META","TSLA","BRK","JPM","V","UNH","XOM",
    "JNJ","WMT","PG","MA","HD","CVX","LLY","MRK","ABBV","PEP","KO","AVGO","COST",
    "MCD","TMO","ABT","ACN","DHR","LIN","VZ","NEE","TXN","PM","RTX","HON","UPS",
    "BMY","AMGN","QCOM","T","SBUX","GS","BLK","LOW","SPGI","AXP","MS","INTU",
    "DE","CAT","ELV","MDT","GILD","ISRG","ADI","REGN","NOW","LRCX","MO","ZTS",
    "CL","SO","DUK","SYK","BDX","CI","AON","CB","USB","PNC","TFC","WFC","BAC","C",
    "SPY","QQQ","GLD","SLV","USO","BTC","ETH",
])

def classify_article(title: str, summary: str) -> str:
    text = (title + " " + summary).lower()
    scores = {cat: sum(1 for kw in kws if kw in text) for cat, kws in CATEGORY_KEYWORDS.items()}
    best = max(scores, key=scores.get)
    return best if scores[best] > 0 else "general"

def extract_tickers(text: str) -> list:
    words = re.findall(r'\b[A-Z]{2,5}\b', text)
    return list(set(w for w in words if w in SP500_TICKERS))

def score_sentiment(title: str, summary: str) -> dict:
    text = title + ". " + summary
    scores = analyzer.polarity_scores(text)
    compound = scores["compound"]
    if compound >= 0.05:
        label = "positive"
    elif compound <= -0.05:
        label = "negative"
    else:
        label = "neutral"
    return {"label": label, "score": round(compound, 3)}

# â”€â”€ Database â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_db():
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS articles (
                id TEXT PRIMARY KEY,
                source TEXT,
                title TEXT,
                url TEXT,
                summary TEXT,
                published TEXT,
                category TEXT,
                sentiment_label TEXT,
                sentiment_score REAL,
                tickers TEXT
            )
        """)
        conn.commit()

def insert_article(a: dict):
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            INSERT OR IGNORE INTO articles
            (id, source, title, url, summary, published, category, sentiment_label, sentiment_score, tickers)
            VALUES (?,?,?,?,?,?,?,?,?,?)
        """, (a["id"], a["source"], a["title"], a["url"], a["summary"],
              a["published"], a["category"], a["sentiment_label"],
              a["sentiment_score"], json.dumps(a["tickers"])))
        conn.commit()

# â”€â”€ Scraping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_all():
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Scraping feeds...")
    count = 0
    for source, url in RSS_FEEDS.items():
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries[:20]:
                article_id = hashlib.md5(entry.get("link", entry.get("title", "")).encode()).hexdigest()
                title = entry.get("title", "").strip()
                summary = re.sub(r'<[^>]+>', '', entry.get("summary", "")).strip()[:400]
                link = entry.get("link", "")

                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    pub = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).isoformat()
                else:
                    pub = datetime.now(timezone.utc).isoformat()

                category = classify_article(title, summary)
                sentiment = score_sentiment(title, summary)
                tickers = extract_tickers(title + " " + summary)

                insert_article({
                    "id": article_id, "source": source, "title": title,
                    "url": link, "summary": summary, "published": pub,
                    "category": category, "sentiment_label": sentiment["label"],
                    "sentiment_score": sentiment["score"], "tickers": tickers
                })
                count += 1
        except Exception as e:
            print(f"  Error [{source}]: {e}")
    print(f"  Done. Processed {count} articles.")

# â”€â”€ API Routes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/feed")
def get_feed():
    category = request.args.get("category")
    ticker = request.args.get("ticker", "").upper()
    keyword = request.args.get("keyword", "").lower()
    sentiment = request.args.get("sentiment")
    limit = min(int(request.args.get("limit", 100)), 200)

    query = "SELECT * FROM articles WHERE 1=1"
    params = []

    if category and category != "all":
        query += " AND category = ?"
        params.append(category)
    if sentiment and sentiment != "all":
        query += " AND sentiment_label = ?"
        params.append(sentiment)
    if ticker:
        query += " AND tickers LIKE ?"
        params.append(f'%"{ticker}"%')
    if keyword:
        query += " AND (LOWER(title) LIKE ? OR LOWER(summary) LIKE ?)"
        params += [f"%{keyword}%", f"%{keyword}%"]

    query += " ORDER BY published DESC LIMIT ?"
    params.append(limit)

    with sqlite3.connect(DB_PATH) as conn:
        conn.row_factory = sqlite3.Row
        rows = conn.execute(query, params).fetchall()

    articles = []
    for r in rows:
        articles.append({
            "id": r["id"], "source": r["source"], "title": r["title"],
            "url": r["url"], "summary": r["summary"], "published": r["published"],
            "category": r["category"], "sentiment_label": r["sentiment_label"],
            "sentiment_score": r["sentiment_score"],
            "tickers": json.loads(r["tickers"] or "[]")
        })

    return jsonify(articles)

@app.route("/api/stats")
def get_stats():
    with sqlite3.connect(DB_PATH) as conn:
        total = conn.execute("SELECT COUNT(*) FROM articles").fetchone()[0]
        by_cat = dict(conn.execute(
            "SELECT category, COUNT(*) FROM articles GROUP BY category").fetchall())
        by_sent = dict(conn.execute(
            "SELECT sentiment_label, COUNT(*) FROM articles GROUP BY sentiment_label").fetchall())
        latest = conn.execute(
            "SELECT published FROM articles ORDER BY published DESC LIMIT 1").fetchone()

    return jsonify({
        "total": total,
        "by_category": by_cat,
        "by_sentiment": by_sent,
        "last_updated": latest[0] if latest else None
    })

@app.route("/api/refresh", methods=["POST"])
def manual_refresh():
    threading.Thread(target=scrape_all).start()
    return jsonify({"status": "scraping started"})

@app.route("/")
def index():
    return send_from_directory(".", "index.html")

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    init_db()
    scrape_all()  # Initial scrape

    scheduler = BackgroundScheduler()
    scheduler.add_job(scrape_all, "interval", minutes=5)
    scheduler.start()

    print("\nðŸš€ Market News Aggregator running at http://localhost:5000\n")
    app.run(debug=False, port=5000, use_reloader=False)
